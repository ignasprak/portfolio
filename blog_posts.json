[
  {
    "id": "2025-08-09-defintely-the-final-test",
    "title": "defintely the final test",
    "date": "2025-08-09",
    "html_content": "<p>oh boy\ncmon lad\nlets try a little update\nlets try another one\nmaybe another one</p>"
  },
  {
    "id": "2025-08-09-data-engineering---day-1",
    "title": "Data Engineering - Day 1",
    "date": "2025-08-09",
    "html_content": "<p>Today I start my first day of the DataTalksClub course that I will be doing on my own, without any intention of getting a certificate and purely to learn about the fundamentals of data engineering.</p>"
  },
  {
    "id": "2025-08-11-datatalksclub---day-1",
    "title": "DataTalksClub - Day 1",
    "date": "2025-08-11",
    "html_content": "<h1>dataengineering #datatalksclub</h1>\n<h1>Intro to Docker (1.2.1)</h1>\n<p>ci/cd is not covered in the course, make sure to look at this in a later date</p>\n<p>docker is useful, because\n- local experimentation\n- integration testing (CI/CD)\n- reproducibility\n- running pipelines on the cloud (AWS batch, kubernetes)\n- Spark (defining data pipelines, used to define dependencies)\n- Serverless (AWS Lambda, processing data one record at a time)</p>\n<h2>Docker Basics:</h2>\n<p><code>docker run -it ubuntu bash</code></p>\n<ul>\n<li>docker is the base command for using docker</li>\n<li>run means to execute an image </li>\n<li>-it means to run in an interactive terminal</li>\n<li>ubuntu means the name of the image we want to run</li>\n<li>bash means the command we want to execute in this image (parameter)\nrunning this image and deleting everything and running it again returns it to its original state because it is isolated</li>\n</ul>\n<p><code>docker run -it python:3.9</code></p>\n<ul>\n<li>python is the name of the image</li>\n<li>3.9 is the tech, otherwise known as the version</li>\n</ul>\n<p><code>docker run -it --entrypoint=bash python:3.9</code>\nin order to install modules, we can use pip install pandas, but in order to do this we need to go to a bash terminal since we cannot do this in python</p>\n<h2>Docker in VSC</h2>\n<p>when we are creating our own container we need specifications for our dockerfile, so we start with the base image</p>\n<p>in this case it would be:\n<code>FROM python:3.9</code></p>\n<p>then we can do a run command which would be \n<code>RUN pip install pandas</code></p>\n<p>since we can only install pandas in bash and not python we do:\n<code>ENTRYPOINT [ \"bash\" ]</code></p>\n<p>in order to build this image from VSC, using the docker file we do:\n<code>docker build -t test:pandas .</code></p>\n<ul>\n<li>build tells docker to build the image</li>\n<li>. means to build it in the current directory</li>\n</ul>\n<p>then when that is done, you can run it by using (which will being you into the bash terminal):\n<code>docker run -it test:pandas</code></p>\n<h2>Data Pipeline</h2>\n<p>We can now create a python file (.py)</p>\n<p>we can type in:\n<code>import pandas as pd</code> \nwhatever fancy stuff it will be doing\n<code>print('yay job done')</code></p>\n<p>then in our docker file we add:\n<code>COPY pipeline.py pipeline.py</code>\nwhich copies the file to docker container</p>\n<p>and we can specify the work directory:\n<code>WORKDIR /app</code></p>\n<p>now you run the container, and go into the python terminal and you can do pipeline.py when you are in the working directory</p>\n<h3>Data Pipeline - Automation</h3>\n<p>to the pipeline file we add:\n<code>import sys</code>\n<code>print(sys.argv)</code>\n<code>day = sys.argv[1]</code>\n<code>print(f'job done good for day = {day}')</code></p>\n<p>then in the docker file we do:\n<code>ENTRYPOINT [ \"python\", \"pipeline.py\" ]</code></p>\n<p>you build the container, and then run it:\n<code>docker run -it test:pandas 2025-08-10</code></p>\n<p>this is how paramterise the data pipeline scripts\n<code>docker run -it test:pandas 2025-08-10 123 hello</code></p>\n<h1>Ingesting NY Taxi Data to Postgres (1.2.2)</h1>\n<h3>docker compose</h3>\n<ul>\n<li>we have environment variables, we use a -e flag for this (postgres_user/password/db)</li>\n<li>we have volumes (a way of mapping folder in host machine file system to a folder in a container), this is called mounting, we use a -v flag for this (needs a full path on window machines) ((for mac you can do in the example $(pwd)/blahblahblah))</li>\n<li>we also need to specify a port on our host machine to a port on the container (needed to send a request to the db, we use a -p for this (5432:5432)</li>\n</ul>\n<p>from the tutorial section we put the code into the terminal to run it\n<code>docker run -it \\</code>\n        <code>-e POSTGRES_USER=\"root\" \\</code>\n        <code>-e POSTGRES_PASSWORD=\"root\" \\</code>\n        <code>-e POSTGRES_DB=\"ny_taxi\" \\</code>\n        <code>-v \"/Users/ignasprakapas/Coding Projects/data-engineering/data-engineering-zoomcamp/01-docker-terraform/2_docker_sql/ny_taxi_postgres_data\":/var/lib/postgresql/data \\</code>\n        <code>-p 5432:5432 \\</code>\n        <code>postgres:13</code>\nNow we want to access the database (pip install pgcli)\n<code>pgcli -h localhost -p 5432 -u root -d ny_taxi</code></p>\n<p>sometimes the port 5432 is already in use by a previous container, so on mac we do <code>sudo lsof -i -P | grep LISTEN | grep :$PORT</code> to list the ports in use and then do <code>sudo kill -9 (PID)</code></p>\n<p>when we are in the container using pgcli, we can do:\n<code>\\dt</code> - to list the tables in there</p>\n<p>we are going to use jupyter now, to install we can do <code>pip install jupyter</code></p>\n<p>then we can do jupyter notebook</p>\n<p>.parquet and not .csv anymore -&gt; just replace read_csv with read_parquet and remove the nrows argument</p>\n<p>when looking at a .csv file you can look at it by using the <code>less</code> command</p>\n<p>some handy data commands\n- head -n 100 xyz.csv &gt; xyz_head.csv = means get the top 100 lines and convert that into a new file\n- wc -l xyz.csv = counts the amount of rows in the .csv file, the -l specifies lines</p>\n<p>the dataset:\nhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet</p>\n<p>for the dataset we are using they also have documentation about it:\nhttps://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf</p>\n<p>there is also a zone id .csv file\n<a href=\"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\">Taxi Zone Lookup Table</a></p>\n<p>So since the file was in .parquet file and not .csv file, we had to use .parquet and work with that in jupyter</p>\n<p><code>import pandas as pd</code>\n<code>!pip install pyarrow</code>\n<code>import urllib.request</code>\n<code>import pyarrow.parquet as pq</code></p>\n<p><code>url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet'</code>\n<code>filename = 'yellow_tripdata_2021-01.parquet'</code>\n<code>urllib.request.urlretrieve(url, filename)</code>\n<code>print(\"File downloaded\")</code></p>\n<p><code>parquet_file = pq.ParquetFile(filename)</code>\n<code>trips_df = parquet_file.read(use_pandas_metadata=True).slice(0, 100).to_pandas()</code></p>\n<p><code>trips_df</code></p>\n<p>Now we want to put this data into our postgres, and to start off we need to turn this into a schema, first we make this into a data definition language (which is used for specifying schemas in sql)</p>\n<p><code>print(pd.io.sql.get_schema(trips_df, name=\"yellow_taxi_data\"))</code></p>\n<p>we notice in the schema that what are meant to be timestamps are in a text format so we need to convert them into timestamp</p>\n<p><code>trips_df.tpep_pickup_datetime = pd.to_datetime(trips_df.tpep_pickup_datetime)</code>trips_df.tpep_dropoff_datetime = pd.to_datetime(trips_df.tpep_dropoff_datetime)`</p>\n<p>this would go above the ddl schema conversion</p>\n<p>then we need to import sqlalchemy</p>\n<p>then we do:\n`from sqlalchemy import create_engine</p>\n<p><code>engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')</code></p>\n<p><code>engine.connect()</code></p>\n<p>then we edit our schema ddl code:\n<code>print(pd.io.sql.get_schema(trips_df, name=\"yellow_taxi_data\", con=engine))</code></p>\n<p>so since this is a large dataset and we are only really inserting the first 100 data points, we can do batch processing, using an iterator</p>\n<p>we create the iterator here:\n<code>parquet_file = pq.ParquetFile('yellow_tripdata_2021-01.parquet')</code>df_iter = parquet_file.iter_batches(batch_size=100000)`</p>\n<p>get the first chunk:\n<code>df = next(df_iter).to_pandas()</code></p>\n<p>check length:\n<code>len(df)</code></p>\n<p>to make sure the data type is timestamps:\n<code>trips_df.tpep_pickup_datetime = pd.to_datetime(trips_df.tpep_pickup_datetime)</code>trips_df.tpep_dropoff_datetime = pd.to_datetime(trips_df.tpep_dropoff_datetime)`</p>\n<p>now we can use:\n<code>df.head(n=0)</code></p>\n<p>this will show us the headers of the data set</p>\n<p>now what we want to do is insert the table and then the data chunk by chunk</p>\n<p>there is a function in dataframes which is called to_sql\n<code>trips_df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace')</code></p>\n<p>the replace keyword will replace a row if its already there with the new row (it will drop the table before inserting new values)</p>\n<p>this command inserts the tables into our postgres in docker, this was produced by Claude AI\n`%time df_iter = parquet_file.iter_batches(batch_size=100000)</p>\n<p><code>for i, batch in enumerate(df_iter):</code>chunk_df = batch.to_pandas()`</p>\n<pre><code># Convert datetime columns\nchunk_df.tpep_pickup_datetime = pd.to_datetime(chunk_df.tpep_pickup_datetime)\nchunk_df.tpep_dropoff_datetime = pd.to_datetime(chunk_df.tpep_dropoff_datetime)\n\nprint(f\"Inserting chunk {i+1} with {len(chunk_df)} rows...\")\n\n# Insert chunk\n%time chunk_df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n\nprint(f\"Finished inserting chunk {i+1}\")\n</code></pre>\n<p>This concludes my learning for the day!</p>"
  }
]